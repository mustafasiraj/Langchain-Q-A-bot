import os
import sys
import streamlit as st

# âœ… Ensure local modules folder is in path
sys.path.append(os.path.dirname(__file__))

# âœ… Import your QA chain from modules
from modules.qa_chain import load_pdf_and_create_qa

# âœ… Set your OpenAI key securely
os.environ["OPENAI_API_KEY"] = "ADD_YOUR_OWN_API_KEYYY"

# âœ… Streamlit config
st.set_page_config(page_title="ğŸ“„ LangChain PDF Q&A Bot", page_icon="ğŸ¤–")
st.title("ğŸ“„ LangChain PDF Q&A AI Agent")

# âœ… Session state for chat history
if "history" not in st.session_state:
    st.session_state.history = []

# âœ… Upload PDF files
uploaded_files = st.file_uploader("Upload one or more PDF files", type="pdf", accept_multiple_files=True)

# âœ… LLM Backend choice (future expansion)
llm_choice = st.selectbox("Choose LLM Backend", ["OpenAI"])

# âœ… Ask a question
question = st.text_input("Ask a question about your PDFs")

# âœ… Clear chat button
if st.button("ğŸ—‘ï¸ Clear Chat"):
    st.session_state.history = []
    st.experimental_rerun()

# âœ… Process the question and show the answer
if uploaded_files and question:
    with st.spinner("Processing... Please wait."):
        try:
            qa, sources = load_pdf_and_create_qa(uploaded_files, llm_choice)
            response = qa({"query": question})
            answer = response["result"]
            sources = response.get("source_documents", [])
            st.session_state.history.append((question, answer, sources))
        except Exception as e:
            st.error(f"âŒ Error: {str(e)}")

# âœ… Display chat history
if st.session_state.history:
    st.subheader("ğŸ§  Q&A History")
    for q, a, sources in reversed(st.session_state.history):
        st.markdown(f"**Q:** {q}")
        st.markdown(f"**A:** {a}")
        if sources:
            with st.expander("ğŸ“š Sources"):
                for i, s in enumerate(sources, 1):
                    st.markdown(f"**Source {i}:** {s}")
        st.markdown("---")
